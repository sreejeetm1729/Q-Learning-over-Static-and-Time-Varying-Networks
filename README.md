We propose a multi-agent \(Q\)-learning algorithm (\textcolor{winered}{\texttt{VRDQ}}) for static and time-varying networks, which achieves provable collaborative speedups in sample complexity. The method combines two complementary ingredients: (i) operator refinement via batched updates and (ii) streamlined diffusion of updates among neighbors within each batch. These components are decoupled by design, isolating their effects and enabling a simplified finite-time analysis. Notably, the algorithm uses only logarithmic communication rounds, substantially lowering the communication overhead. Although developed for \(Q\)-learning, the framework extends naturally to broader reinforcement-learning settings, highlighting its generality and practicality.
